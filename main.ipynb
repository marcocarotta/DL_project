{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the entry point for the project.\n",
    "The choice of .ipynb it's for speeding up the training using Google Colab or Kaggle machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "In the following code block we instanciate the dataset and use some method for analyse it and preprocess it.\n",
    "\n",
    "### Comments \n",
    "- as of now it's a little bit too verbose the analysis, consider adding a verbose parameter in the dataset_analysis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CharDataset:### Dataset Analysis ###\n",
      "INFO:CharDataset:### Comparison with Original Dataset ###\n",
      "INFO:CharDataset:Original Total Characters: 1115394\n",
      "INFO:CharDataset:Current Total Characters: 1115394\n",
      "INFO:CharDataset:Original Unique Characters: 65\n",
      "INFO:CharDataset:Current Unique Characters: 65\n",
      "INFO:CharDataset:### Detailed Current Dataset Analysis ###\n",
      "INFO:CharDataset:### Characters and Frequencies ###\n",
      "INFO:CharDataset:'␣': 169892\n",
      "INFO:CharDataset:'e': 94611\n",
      "INFO:CharDataset:'t': 67009\n",
      "INFO:CharDataset:'o': 65798\n",
      "INFO:CharDataset:'a': 55507\n",
      "INFO:CharDataset:'h': 51310\n",
      "INFO:CharDataset:'s': 49696\n",
      "INFO:CharDataset:'r': 48889\n",
      "INFO:CharDataset:'n': 48529\n",
      "INFO:CharDataset:'i': 45537\n",
      "INFO:CharDataset:'\\n': 40000\n",
      "INFO:CharDataset:'l': 33339\n",
      "INFO:CharDataset:'d': 31358\n",
      "INFO:CharDataset:'u': 26584\n",
      "INFO:CharDataset:'m': 22243\n",
      "INFO:CharDataset:'y': 20448\n",
      "INFO:CharDataset:',': 19846\n",
      "INFO:CharDataset:'w': 17585\n",
      "INFO:CharDataset:'f': 15770\n",
      "INFO:CharDataset:'c': 15623\n",
      "INFO:CharDataset:'g': 13356\n",
      "INFO:CharDataset:'I': 11832\n",
      "INFO:CharDataset:'b': 11321\n",
      "INFO:CharDataset:'p': 10808\n",
      "INFO:CharDataset:':': 10316\n",
      "INFO:CharDataset:'.': 7885\n",
      "INFO:CharDataset:'A': 7819\n",
      "INFO:CharDataset:'v': 7793\n",
      "INFO:CharDataset:'k': 7088\n",
      "INFO:CharDataset:'T': 7015\n",
      "INFO:CharDataset:''': 6187\n",
      "INFO:CharDataset:'E': 6041\n",
      "INFO:CharDataset:'O': 5481\n",
      "INFO:CharDataset:'N': 5079\n",
      "INFO:CharDataset:'R': 4869\n",
      "INFO:CharDataset:'S': 4523\n",
      "INFO:CharDataset:'L': 3876\n",
      "INFO:CharDataset:'C': 3820\n",
      "INFO:CharDataset:';': 3628\n",
      "INFO:CharDataset:'W': 3530\n",
      "INFO:CharDataset:'U': 3313\n",
      "INFO:CharDataset:'H': 3068\n",
      "INFO:CharDataset:'M': 2840\n",
      "INFO:CharDataset:'B': 2761\n",
      "INFO:CharDataset:'?': 2462\n",
      "INFO:CharDataset:'G': 2399\n",
      "INFO:CharDataset:'!': 2172\n",
      "INFO:CharDataset:'D': 2089\n",
      "INFO:CharDataset:'-': 1897\n",
      "INFO:CharDataset:'F': 1797\n",
      "INFO:CharDataset:'Y': 1718\n",
      "INFO:CharDataset:'P': 1641\n",
      "INFO:CharDataset:'K': 1584\n",
      "INFO:CharDataset:'V': 798\n",
      "INFO:CharDataset:'j': 628\n",
      "INFO:CharDataset:'q': 609\n",
      "INFO:CharDataset:'x': 529\n",
      "INFO:CharDataset:'z': 356\n",
      "INFO:CharDataset:'J': 320\n",
      "INFO:CharDataset:'Q': 231\n",
      "INFO:CharDataset:'Z': 198\n",
      "INFO:CharDataset:'X': 112\n",
      "INFO:CharDataset:'3': 27\n",
      "INFO:CharDataset:'&': 3\n",
      "INFO:CharDataset:'$': 1\n",
      "INFO:CharDataset:### Sequence Analysis ###\n",
      "INFO:CharDataset:Number of Lines: 40001\n",
      "INFO:CharDataset:Average Line Length (in char): 26.88\n",
      "INFO:CharDataset:Max Line Length (in char): 63\n",
      "INFO:CharDataset:### Character Set Changes ###\n",
      "INFO:CharDataset:No changes in character set.\n",
      "INFO:CharDataset:Removed characters with frequency < 10 and replaced them with '@' token.\n",
      "INFO:CharDataset:### Dataset Analysis ###\n",
      "INFO:CharDataset:### Comparison with Original Dataset ###\n",
      "INFO:CharDataset:Original Total Characters: 1115394\n",
      "INFO:CharDataset:Current Total Characters: 1115394\n",
      "INFO:CharDataset:Original Unique Characters: 65\n",
      "INFO:CharDataset:Current Unique Characters: 64\n",
      "INFO:CharDataset:### Detailed Current Dataset Analysis ###\n",
      "INFO:CharDataset:### Characters and Frequencies ###\n",
      "INFO:CharDataset:'␣': 169892\n",
      "INFO:CharDataset:'e': 94611\n",
      "INFO:CharDataset:'t': 67009\n",
      "INFO:CharDataset:'o': 65798\n",
      "INFO:CharDataset:'a': 55507\n",
      "INFO:CharDataset:'h': 51310\n",
      "INFO:CharDataset:'s': 49696\n",
      "INFO:CharDataset:'r': 48889\n",
      "INFO:CharDataset:'n': 48529\n",
      "INFO:CharDataset:'i': 45537\n",
      "INFO:CharDataset:'\\n': 40000\n",
      "INFO:CharDataset:'l': 33339\n",
      "INFO:CharDataset:'d': 31358\n",
      "INFO:CharDataset:'u': 26584\n",
      "INFO:CharDataset:'m': 22243\n",
      "INFO:CharDataset:'y': 20448\n",
      "INFO:CharDataset:',': 19846\n",
      "INFO:CharDataset:'w': 17585\n",
      "INFO:CharDataset:'f': 15770\n",
      "INFO:CharDataset:'c': 15623\n",
      "INFO:CharDataset:'g': 13356\n",
      "INFO:CharDataset:'I': 11832\n",
      "INFO:CharDataset:'b': 11321\n",
      "INFO:CharDataset:'p': 10808\n",
      "INFO:CharDataset:':': 10316\n",
      "INFO:CharDataset:'.': 7885\n",
      "INFO:CharDataset:'A': 7819\n",
      "INFO:CharDataset:'v': 7793\n",
      "INFO:CharDataset:'k': 7088\n",
      "INFO:CharDataset:'T': 7015\n",
      "INFO:CharDataset:''': 6187\n",
      "INFO:CharDataset:'E': 6041\n",
      "INFO:CharDataset:'O': 5481\n",
      "INFO:CharDataset:'N': 5079\n",
      "INFO:CharDataset:'R': 4869\n",
      "INFO:CharDataset:'S': 4523\n",
      "INFO:CharDataset:'L': 3876\n",
      "INFO:CharDataset:'C': 3820\n",
      "INFO:CharDataset:';': 3628\n",
      "INFO:CharDataset:'W': 3530\n",
      "INFO:CharDataset:'U': 3313\n",
      "INFO:CharDataset:'H': 3068\n",
      "INFO:CharDataset:'M': 2840\n",
      "INFO:CharDataset:'B': 2761\n",
      "INFO:CharDataset:'?': 2462\n",
      "INFO:CharDataset:'G': 2399\n",
      "INFO:CharDataset:'!': 2172\n",
      "INFO:CharDataset:'D': 2089\n",
      "INFO:CharDataset:'-': 1897\n",
      "INFO:CharDataset:'F': 1797\n",
      "INFO:CharDataset:'Y': 1718\n",
      "INFO:CharDataset:'P': 1641\n",
      "INFO:CharDataset:'K': 1584\n",
      "INFO:CharDataset:'V': 798\n",
      "INFO:CharDataset:'j': 628\n",
      "INFO:CharDataset:'q': 609\n",
      "INFO:CharDataset:'x': 529\n",
      "INFO:CharDataset:'z': 356\n",
      "INFO:CharDataset:'J': 320\n",
      "INFO:CharDataset:'Q': 231\n",
      "INFO:CharDataset:'Z': 198\n",
      "INFO:CharDataset:'X': 112\n",
      "INFO:CharDataset:'3': 27\n",
      "INFO:CharDataset:'@': 4\n",
      "INFO:CharDataset:### Sequence Analysis ###\n",
      "INFO:CharDataset:Number of Lines: 40001\n",
      "INFO:CharDataset:Average Line Length (in char): 26.88\n",
      "INFO:CharDataset:Max Line Length (in char): 63\n",
      "INFO:CharDataset:### Character Set Changes ###\n",
      "INFO:CharDataset:Characters Removed: {'&', '$'}\n",
      "INFO:CharDataset:New Characters Added: {'@'}\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "import torch\n",
    "from dataset import CharDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "try: \n",
    "    with open('dataset/dataset.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    with open('data/dataset.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "block_size = 128 # spatial extent of the model for its context, so it need to be used in the dataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = CharDataset(text, block_size)\n",
    "\n",
    "# Analyze original dataset\n",
    "dataset.dataset_analysis()\n",
    "\n",
    "# Remove less frequent characters and analyze the dataset\n",
    "dataset.remove_less_frequent_chars(10).dataset_analysis()\n",
    "\n",
    "dataset.preprocess()\n",
    "\n",
    "if 'train_dataset' in globals():\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "else:\n",
    "    loader = DataLoader(dataset, batch_size=2048, shuffle=True)\n",
    "\n",
    "### make this work in the future, as of now just use the whole dataset for training\n",
    "# # Split the dataset into training and validation sets\n",
    "# percentage = 0.9\n",
    "# split_point = int(len(dataset) * percentage)\n",
    "\n",
    "# # Slice the text for training and validation\n",
    "# train_text = text[:split_point]\n",
    "# val_text = text[split_point:]\n",
    "\n",
    "# # doing stupid split for now\n",
    "# train_dataset = CharDataset(train_text, block_size)\n",
    "# val_dataset = CharDataset(val_text, block_size)\n",
    "\n",
    "# # Instantiate DataLoader objects\n",
    "# train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32, 30, 27,  ..., 18,  1, 37],\n",
      "        [27, 34, 17,  ..., 30,  1, 26],\n",
      "        [23, 17,  1,  ..., 17,  1, 32],\n",
      "        ...,\n",
      "        [14, 37,  1,  ..., 37,  6,  0],\n",
      "        [30, 13, 24,  ...,  0, 23, 21],\n",
      "        [ 1, 27, 18,  ..., 27, 18,  1]])\n",
      "tensor([[30, 27, 33,  ...,  1, 37, 27],\n",
      "        [34, 17, 30,  ...,  1, 26, 27],\n",
      "        [17,  1, 34,  ...,  1, 32, 27],\n",
      "        ...,\n",
      "        [37,  1, 20,  ...,  6,  0, 32],\n",
      "        [13, 24,  1,  ..., 23, 21, 26],\n",
      "        [27, 18,  1,  ..., 18,  1, 25]])\n"
     ]
    }
   ],
   "source": [
    "max_index = len(loader)\n",
    "# try the datloader for some random indexes\n",
    "\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    print(x[0])\n",
    "    print(y[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate the model\n",
    "Here we set the least amount of parameter needed for the model and call the model summary method\n",
    "\n",
    "### comments\n",
    "- as of now the model summary doesn't work (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CharTransformer:Model initialized\n",
      "INFO:CharTransformer:### Model summary###\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "CharTransformerSummaryWrapper                 [32, 128, 39]             --\n",
      "├─CharTransformer: 1-1                        [32, 128, 39]             16,384\n",
      "│    └─Embedding: 2-1                         [32, 128, 128]            4,992\n",
      "│    └─Dropout: 2-2                           [32, 128, 128]            --\n",
      "│    └─ModuleList: 2-3                        --                        --\n",
      "│    │    └─TransformerBlock: 3-1             [32, 128, 128]            132,480\n",
      "│    │    └─TransformerBlock: 3-2             [32, 128, 128]            132,480\n",
      "│    └─LayerNorm: 2-4                         [32, 128, 128]            256\n",
      "│    └─Linear: 2-5                            [32, 128, 39]             5,031\n",
      "===============================================================================================\n",
      "Total params: 291,623\n",
      "Trainable params: 291,623\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 8.81\n",
      "===============================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 85.16\n",
      "Params size (MB): 1.10\n",
      "Estimated Total Size (MB): 86.30\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from model import CharTransformer\n",
    "\n",
    "# usually should be train_dataset, but for now just use the whole dataset for training\n",
    "if 'train_dataset' in globals():\n",
    "    vocabulary_size = train_dataset.vocabulary_size\n",
    "else:\n",
    "    vocabulary_size = dataset.vocabulary_size\n",
    "\n",
    "block_size # Sequence length defined earlier as block_size\n",
    "embedding_dim = 128  # Embedding dimensions\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_layers = 2  # Number of transformer blocks\n",
    "\n",
    "# Initialize the model\n",
    "model = CharTransformer(vocabulary_size, block_size, embedding_dim, num_heads, num_layers, ff_hid_dim=256)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import Trainer\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "# Set optimizer and loss function   \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(model, optimizer, loss_fn, loader)\n",
    "\n",
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/model_checkpoint_0_kaggle.pth\n",
      "Oh god oh gododododoooooooooooooooooooooooooooooooooooooooooooodooooooooooooooooooooooooooooooooooooooooooooooooo\n"
     ]
    }
   ],
   "source": [
    "from generate import TextGenerator\n",
    "\n",
    "generator = TextGenerator(model, dataset)\n",
    "\n",
    "generator.load_model(\"models/model_checkpoint_0_kaggle.pth\")\n",
    "# Generate text\n",
    "generated_text = generator.generate(\"Oh god oh god\",length = 100, temperature=0.7)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
